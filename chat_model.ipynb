{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Experimenting with Mistral chat model\n",
    "## Running the notebook\n",
    "The model will be quantized to 4bit, so it can fit into 8GB of VRAM. If you have more than 8GB, experiment with 8bit quantization, or try to load without quantization, but it will require a GPU with 24GB of VRAM.\n",
    "Model link on huggingface: [Mistral-7B-Instruct-v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)\n",
    "\n",
    "**Important: I was only able to get bitsandbytes working on Linux!**\n",
    "If you don't want to struggle with VMs and docker, I suggest you to use Windows Subsystem for Linux: [WSL setup](https://learn.microsoft.com/en-us/windows/wsl/install)\n",
    "If you run this notebook in pycharm professional, you can add WSL interpreter to your project like this: [WSL interpreter in PyCharm Professional](https://www.jetbrains.com/help/pycharm/using-wsl-as-a-remote-interpreter.html)\n",
    "Also, you need these packages but using the WSL interpreter, I was unable to install it using the notebook. So you need to use the python console and install them with these commands: \n",
    "```\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "pip install bitsandbytes\n",
    "pip install transformers\n",
    "```\n",
    "## Terms and technologies explained\n",
    "### What is a chat model?\n",
    "[TODO: Explain what is a chat model]\n",
    "\n",
    "### What is quantization?\n",
    "[TODO: Explain quantization]\n",
    "[Quantization explained](https://www.tensorops.ai/post/what-are-quantized-llms)\n",
    "\n",
    "### How does Mistral model work?\n",
    "[TODO: Explain how Mistral model works]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f905125829da0d8c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad6d8536ec304c22a2f3d55f7b9bbc70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\", load_in_4bit=True, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T20:55:03.321180500Z",
     "start_time": "2024-02-16T20:54:39.393305100Z"
    }
   },
   "id": "ec9d39663be92dd1",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] What is your favourite condiment? [/INST]Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s> [INST] Give me a ketchup recipe. [/INST] I'm glad you asked for a ketchup recipe, even though my personal favourite is fresh lemon juice. Here's a simple homemade ketchup recipe that you can try:\n",
      "\n",
      "Ingredients:\n",
      "- 2 1/2 pounds (1.13 kg) ripe, red tomatoes\n",
      "- 1 1/2 cups (360 ml) cider vinegar\n",
      "- 1 cup (240 g) granulated sugar\n",
      "- 1/2 cup (120 g) chopped onion\n",
      "- 1/2 cup (120 g) chopped green bell pepper\n",
      "- 2 tablespoons (30 ml) grated horseradish (or more to taste)\n",
      "- 1 to 2 jalapeno peppers, seeded and finely chopped (optional, for heat)\n",
      "- 2 teaspoons (10 ml) salt\n",
      "- 1 teaspoon (5 ml) ground allspice\n",
      "- 1 teaspoon (5 ml) ground cinnamon\n",
      "- 1 teaspoon (5 ml) ground cloves\n",
      "- 1/4 teaspoon (1 ml) ground ginger\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Bring a large pot of water to a boil. Core the tomatoes and drop them into the water for 30-60 seconds, then remove with a slotted spoon and place in a large bowl filled with ice water to cool. Once cooled, remove the skins.\n",
      "\n",
      "2. In a large, heavy-bottomed pot, combine vinegar, sugar, onion, bell pepper, horseradish, jalapenos (if using), and salt. Bring to a gentle simmer over medium heat. Cook, stirring occasionally, until the onion and bell pepper are softened, about 20 minutes.\n",
      "\n",
      "3. Once the onion and bell pepper are cooked, add the tomatoes and their juices, along with the remaining ingredients. Bring the mixture to a simmer and cook, stirring frequently, for 1 hour. Adjust seasoning to taste with more salt, if needed.\n",
      "\n",
      "4. Transfer the ketchup to a blender or food processor and process into a smooth puree. Alternatively, you may choose to leave the ketchup chunky.\n",
      "\n",
      "5. Let cool to room temperature, then use a funnel to transfer the ketchup into a sterilised container. Store in the refrigerator for up to 6 months. This recipe makes about 4 1/2 cups (1.06 kg) ketchup.</s>\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Give me a ketchup recipe.\"}\n",
    "]\n",
    "\n",
    "encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T20:57:19.129789600Z",
     "start_time": "2024-02-16T20:56:48.026696400Z"
    }
   },
   "id": "eb436ade2d725e00",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e1c409b63ed1685"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
